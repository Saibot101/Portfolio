import{C as m,P as g,b as f,c as y}from"./VList.a8f27c7e.js";import{_ as b,i as w,ae as h,k as t,ak as x,l as i,h as e,p as s,j as k,ah as v,F as L,m as n,al as p,t as D,aj as u}from"./index.fe645947.js";import"./forwardRefs.8760e9ba.js";import"./getScrollParent.ff47518e.js";const q={components:{CodeSnippet:m,ParagraphSnippet:g},methods:{scroll(c){document.getElementById(c).scrollIntoView({behavior:"smooth"})}},setup(){w({title:"Creating a vectorstore from PDFs and query the store in different ways",meta:{description:"This tutorial shows how to query a vector store in different ways with LLM. In the first step, vectorstore is created and loaded with the data from the PDF. Then different ways are shown to query the vectorstore."}})},data(){return{list_inhalt:[{name:"Introduction",scroll:"introduction"},{name:"Packages and prepare PDFs",scroll:"create"},{name:"Load Store",scroll:"load"},{name:"Similarity",scroll:"similarity"},{name:"VectorDBQA",scroll:"vectorDBQA"},{name:"QA with Sources",scroll:"qa_with_sources"}],packages:["from langchain.embeddings import HuggingFaceEmbeddings","from langchain.vectorstores import DeepLake","from langchain.text_splitter import CharacterTextSplitter","from langchain.document_loaders import PagedPDFSplitter"],api:["import os",'os.environ["HUGGINGFACEHUB_API_TOKEN"]="api_key"'],load_data:["def load_reports(urls):","  pages = []"," ","  for url in urls:","    loader = PagedPDFSplitter(url)","    local_pages = loader.load_and_split()","    pages.extend(local_pages)","  return pages","",'urls = ["1.pdf","2.pdf"]',"pages = load_reports(urls)"],text:["text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)"," ","texts =text_splitter.split_documents(pages)"],create:['model_name = "sentence-transformers/all-mpnet-base-v2"',"embeddings = HuggingFaceEmbeddings(model_name=model_name)"," ",'db = DeepLake.from_documents(texts,embeddings, dataset_path="./db/")'],load:['db = DeepLake(dataset_path="./db/", embedding_function=embeddings)'],similarity:['query = "question"',"docs = db.similarity_search(query)","print(docs[0].page_content)"],vectordbqa:["from langchain import VectorDBQA","from langchain import HuggingFaceHub",'qa = VectorDBQA.from_chain_type(llm=HuggingFaceHub(repo_id="google/flan-t5-xl"),chain_type="stuff",vectorstore=db)'," ",'print(qa.run("question"))'],qa_with_sources:["from langchain.chains.qa_with_sources import load_qa_with_sources_chain"," ",'chain = load_qa_with_sources_chain(HuggingFaceHub(repo_id="google/flan-t5-xl"), chain_type="stuff")','query = "question"','print(chain({"input_documents": docs, "question": query}, return_only_outputs=True))']}}},F=n("h1",{class:"text-center mt-10 mb-5 text-h3"}," Creating Vectorstore with PDFs and querying vector store on different ways ",-1),P=n("h2",{class:"text-h4"},"Contents",-1),T=n("h2",{class:"text-h4"},"Introduction",-1),S=n("h2",{class:"text-h4"},"Packages and prepare PDFs",-1),C=n("h2",{class:"text-h4"},"Create Store",-1),j=n("h2",{class:"text-h4"},"Load Store",-1),B=n("h2",{class:"text-h4"},"Similarity",-1),A=n("h2",{class:"text-h4"},"VectorDBQA",-1),H=n("h2",{class:"text-h4"},"QA with Sources",-1);function V(c,I,E,Q,a,_){const r=p("ParagraphSnippet"),o=p("CodeSnippet");return i(),h(x,null,{default:t(()=>[F,e(s,{justify:"center",class:"mt-10"},{default:t(()=>[P]),_:1}),e(s,{justify:"center"},{default:t(()=>[e(f,{dense:""},{default:t(()=>[(i(!0),k(L,null,v(a.list_inhalt,(l,d)=>(i(),h(y,{key:d,onClick:M=>_.scroll(l.scroll)},{default:t(()=>[D(u(d+1)+". "+u(l.name),1)]),_:2},1032,["onClick"]))),128))]),_:1})]),_:1}),e(s,{justify:"center",id:"introduction"},{default:t(()=>[T]),_:1}),e(r,{paragraph:"Langchain is the most used and well-known package for building software around LLMs. It provides an easy-to-use interface to load different data formats, store data into different vector database, use different Embeddings and query different LLMs. Therefore, this package is the logical start for a project in that direction. "},null,8,["paragraph"]),e(s,{justify:"center",id:"create"},{default:t(()=>[S]),_:1}),e(r,{paragraph:"The embeddings will be queried from HuggingFace and as a vector store will be DeepLake used. DeepLake markets itself as a Data Lake for Deep Learning."},null,8,["paragraph"]),e(o,{code_array:a.packages},null,8,["code_array"]),e(r,{paragraph:"The API-Key to HuggingFace needs to be stored to the user environment so that it can be read from there."},null,8,["paragraph"]),e(o,{code_array:a.api},null,8,["code_array"]),e(r,{paragraph:"The reading of the PDFs is handled in short function. The PDFs are loaded by a preprogrammed function from Langchain. Then the PDFs will be stored in as a Document object."},null,8,["paragraph"]),e(o,{code_array:a.load_data},null,8,["code_array"]),e(r,{paragraph:"Because the PDFs have to much text, they must be split into smaller chunks. Therefore, chunks with length of 1000 are created."},null,8,["paragraph"]),e(o,{code_array:a.text},null,8,["code_array"]),e(s,{justify:"center",id:"create"},{default:t(()=>[C]),_:1}),e(r,{paragraph:"Before the store can be created, embeddings need to be downloaded. The embeddings have a crucial job, because the determine the vector representation of the text. With the chunks of texts and the embeddings, the DeepLake vector store is created and saved to the computer."},null,8,["paragraph"]),e(o,{code_array:a.create},null,8,["code_array"]),e(s,{justify:"center",id:"load"},{default:t(()=>[j]),_:1}),e(r,{paragraph:"The vector store can also be loaded from the computer. That makes it easy to seperat adding files and using it in production. Also, it allows to versioning the vector store."},null,8,["paragraph"]),e(o,{code_array:a.load},null,8,["code_array"]),e(s,{justify:"center",id:"similarity"},{default:t(()=>[B]),_:1}),e(r,{paragraph:"The vector store can return the most similar chunk of text to an answer. For that, multiple distance functions can be used."},null,8,["paragraph"]),e(o,{code_array:a.similarity},null,8,["code_array"]),e(s,{justify:"center",id:"vectorDBQA"},{default:t(()=>[A]),_:1}),e(r,{paragraph:"A LLM can be used to answer in logical sentences. For that, the vector store is searched for the most similar chunk of text. With the text a LLM get called to answer the question. The LLM uses only the chunk of text as resource to answer."},null,8,["paragraph"]),e(o,{code_array:a.vectordbqa},null,8,["code_array"]),e(s,{justify:"center",id:"qa_with_sources"},{default:t(()=>[H]),_:1}),e(r,{paragraph:"It is also possible, to let you return the source, from where the text originally comes from. That is especially helpful, if there are many documents in the vector store or try to check, whether the answer is correct."},null,8,["paragraph"]),e(o,{code_array:a.qa_with_sources},null,8,["code_array"])]),_:1})}const W=b(q,[["render",V]]);export{W as default};
